---
title: "P8106 Midterm Project"
author: "Leyang Rui, Jinghan Zhao"
date: "2025-03-28"
output:
  pdf_document: default
  html_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
set.seed(37)

library(tidyverse)
library(forcats)
library(corrplot)
library(caret)
library(mgcv)
library(earth)
library(rpart)
library(rpart.plot)
library(party)
library(partykit)

knitr::opts_chunk$set(fig.width = 10, 
                      fig.height = 5,
                      out.width = "90%",
                      message = FALSE,
                      warning = FALSE)
theme_set(theme_bw())
```

## Load Data

```{r load_data}
load("data/dat1.RData")
train_data = dat1 |>
  janitor::clean_names() |>
  mutate(
    gender = as.factor(gender),
    diabetes = as.factor(diabetes),
    hypertension = as.factor(hypertension),
    race = fct_recode(race,
                  White = "1",
                  Asian = "2",
                  Black = "3",
                  Hispanic = "4"),
    gender = fct_recode(gender,
                        Male = "1",
                        Female = "0"),
    smoking = fct_recode(smoking,
                         "Never smoked" = "0",
                         "Former smoker" = "1",
                         "Current smoker" = "2"))

load("data/dat2.RData")
test_data = dat2 |>
  janitor::clean_names() |>
  mutate(
    gender = as.factor(gender),
    diabetes = as.factor(diabetes),
    hypertension = as.factor(hypertension),
    race = fct_recode(race,
                  White = "1",
                  Asian = "2",
                  Black = "3",
                  Hispanic = "4"),
    gender = fct_recode(gender,
                        Male = "1",
                        Female = "0"),
    smoking = fct_recode(smoking,
                         "Never smoked" = "0",
                         "Former smoker" = "1",
                         "Current smoker" = "2")
  )

```

### Modify Data

```{r modify_data}
train_data1 = 
  train_data %>% 
  select(-id, -height, -weight, -hypertension)

x_train = model.matrix(log_antibody ~ ., train_data1)[, -1]
colnames(x_train) = make.names(colnames(x_train), unique = TRUE)
y_train = train_data1[, "log_antibody"]

test_data = 
  test_data %>% 
  select(-id, -height, -weight, -hypertension)

x_test = model.matrix(log_antibody ~ ., test_data)[, -1]
colnames(x_test) = make.names(colnames(x_test), unique = TRUE)
y_test = test_data[, "log_antibody"]

ctrl1 = trainControl(method = "cv", number = 10)
```

## Descriptive Analysis

### Numeric Variables

```{r numeric_demo_plot, fig.cap="Distribution of the Demographic Continuous Variables"}
train_data |> 
  pivot_longer(
    cols = c(age, height, weight, bmi),
    names_to = "variable",
    values_to = "value"
  ) |>
  ggplot(aes(x = value, y = log_antibody, color = variable)) +
  geom_point(alpha = 0.5, size = 0.6) +
  facet_wrap(variable ~ .,  scales = "free") +
    labs(title = "Distribution of the Demographic Continuous Variables",
         x = "Variables",
         y = "Log_Antibody")
```

```{r numeric_cli_plot, fig.cap="Distribution of the Clinical Continuous Variables"}
train_data |> 
  pivot_longer(
    cols = c(sbp, ldl, time),
    names_to = "variable",
    values_to = "value"
  ) |>
  ggplot(aes(x = value, y = log_antibody, color = variable)) +
  geom_point(alpha = 0.5, size = 0.6) +
  facet_wrap(variable ~ .,  scales = "free") +
    labs(title = "Distribution of the Clinical Continuous Variables",
         x = "Variables",
         y = "Log_Antibody")
```


```{r numeric_table}
train_data %>% 
  pivot_longer(
    cols = c(age, height, weight, bmi, sbp, ldl, time, log_antibody),
    names_to = "variable_name",
    values_to = "value"
  ) %>% 
  group_by(variable_name) %>% 
  summarize(
    mean = mean(value),
    median = median(value),
    min = min(value),
    first_quantile = quantile(value, probs = 0.25),
    third_quantile = quantile(value, probs = 0.75),
    max = max(value)
  ) %>% 
  ungroup() %>% 
  arrange(desc(variable_name == "log_antibody"), variable_name) %>% 
  knitr::kable(digits = 3, caption = "Descriptive Statistics for the Continuous Variables")
```


### Categorical Variables

```{r categorical_plot, fig.cap="Distribution of the Categorical Variables"}
train_data |> 
  pivot_longer(
    cols = c(gender, race, smoking, diabetes, hypertension),
    names_to = "variable",
    values_to = "value"
  ) |>
  mutate(
    variable = factor(variable, levels = c("gender", "race", "smoking", "diabetes", "hypertension"))
  ) |>
  ggplot(aes(x = value, y = log_antibody, fill = variable)) +
  geom_boxplot(alpha = 0.5) +
  facet_wrap(variable ~ .,  scales = "free") +
    labs(title = "Distribution of the Categorical Variables",
         x = "Variables",
         y = "Log_Antibody") +
  theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1))
```


### Correlation Plot

```{r corr_plot, fig.cap="Correlation Plot"}
x_corr = model.matrix(log_antibody ~ ., train_data[, -1])[, -1]
corrplot(cor(x_corr), method = "circle", type = "full")
```



## Regression


### Elastic Net

```{r enet_fit}
set.seed(37)
enet_fit = train(log_antibody ~ ., 
                 data = train_data1, 
                 method = "glmnet", 
                 tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
                                        lambda = exp(seq(-2, -8, length = 100))),
                 trControl = ctrl1)
enet_fit$bestTune
coef(enet_fit$finalModel, enet_fit$bestTune$lambda)
```

```{r enet_plot, fig.cap="Effect of Tuning Parameters on Train Error (Elastic Net)"}
mycol = rainbow(25)
mypar = list(superpose.symbol = list(col = mycol),
superpose.line = list(col = mycol))

plot(enet_fit, par.settings = mypar, xTrans = log)
```

### PCR

```{r pcr_fit}
set.seed(37)
pcr_fit = train(x_train, y_train,
                method = "pcr",
                tuneGrid = data.frame(ncomp = 1:12),
                trControl = ctrl1,
                preProcess = c("center", "scale"))
summary(pcr_fit)
```

```{r pcr_plot, fig.cap="Component Selection (PCR)"}
ggplot(pcr_fit, highlight = TRUE)
```

### PLS

```{r pls_fit}
set.seed(37)
pls_fit = train(x_train, y_train,
                method = "pls", 
                tuneGrid = data.frame(ncomp = 1:12),
                trControl = ctrl1,
                preProcess = c("center", "scale"))
summary(pls_fit)
```

```{r pls_plot, fig.cap="Component Selection (PLS)"}
ggplot(pls_fit, highlight = TRUE)
```


### GAM

```{r, GAM, fig.width=10, fig.height = 8, fig.align='center', out.width='90%', fig.cap="Degree of Predictors (GAM)"}
set.seed(37)

gam.fit = train(x_train, y_train,
                 method = "gam", 
                 trControl = ctrl1)

gam.fit$bestTune
gam.fit$finalModel

par(mfrow = c(3, 2))
plot(gam.fit$finalModel)

par(mfrow = c(1, 1))
```

### MARS

```{r, MARS, fig.cap="Term and Degree Selection (MARS)"}
set.seed(37)

mars_grid = expand.grid(degree = 1:3,
                        nprune = 2:12)

mars.fit = train(x_train, y_train,
                 method = "earth",
                 tuneGrid = mars_grid,
                 trControl = ctrl1)

ggplot(mars.fit)

mars.fit$bestTune
coef(mars.fit$finalModel)
```

### Regression Trees

```{r regression tree, fig.cap="Tuning Parameter Selection (Regression Tree)", warning=FALSE}
set.seed(37)

tree.fit = train(x_train, y_train, 
                 method = "rpart",
                 tuneGrid = data.frame(cp = exp(seq(-10, 0, length = 100,))),
                 trControl = ctrl1)

plot(tree.fit, xTrans = log)
```

```{r, fig.cap="Final Regression Tree Model"}
rpart.plot(tree.fit$finalModel)
```


### Comparison

```{r compare, fig.cap="Model Selection"}
resamp = resamples(list(elastic_net = enet_fit,
                        pcr = pcr_fit,
                        pls = pls_fit,
                        gam = gam.fit,
                        mars = mars.fit,
                        tree = tree.fit))
summary(resamp)
bwplot(resamp, metric = "RMSE")
```

## Model Performance

```{r, pre_vs_res, fig.width=10, fig.height = 8, out.width='90%', fig.cap="Residuals vs Fitted Values (MARS)"}
predicted_values = predict(mars.fit$finalModel, newdata = x_test)

residuals = y_test - predicted_values

plot(predicted_values, residuals, 
     xlab = "Fitted Values", ylab = "Residuals",
     main = "Residuals vs Fitted Values (MARS)",
     pch = 20, col = "mediumpurple1")
abline(h = 0, col = "blue", lwd = 2)
```

```{r, pre_vs_act, fig.width=10, fig.height = 8, out.width='90%', fig.cap="Prediction vs Actual (MARS)"}
plot(y_test, predicted_values,
     xlab = "Actual Values", ylab = "Predicted Values",
     main = "Prediction vs Actual (MARS)",
     pch = 20, col = "orange")
abline(0, 1, col = "mediumseagreen", lwd = 2)
```

```{r test_error}
rmse = sqrt(mean((y_test - predicted_values)^2))
rmse
```

